{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "ds-N423a.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/inthemingcha/ds-admission-challenge/blob/master/ds-section4-sprint2/N423/ds_N423a.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pBQsZEJmubLs"
      },
      "source": [
        "<img src='https://user-images.githubusercontent.com/6457691/90080969-0f758d00-dd47-11ea-8191-fa12fd2054a7.png' width = '200' align = 'right'>\n",
        "\n",
        "## *DATA SCIENCE / SECTION 4 / SPRINT 2 / Assingment 3*\n",
        "\n",
        "---\n",
        "# Neural Network Framework (Keras)\n",
        "\n",
        "\n",
        "## **케라스 라이브러리**를 사용하여 Multi-Layer Perceptron 모델을 CIFAR100 데이터에 적용해보세요.\n",
        "\n",
        "- 케라스에서 필요한 **모델** 및 **레이어의 유형(출력층 재설정 필요)**을 가져오십시오.\n",
        "- 적합한 활성함수 사용\n",
        "- 모델 컴파일\n",
        "- ***피쳐 엔지니어링 후*** 어느 모델이 **새로운 피쳐로 인해 정확도가 더 높아졌는가?**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SBPLbaggP52G",
        "outputId": "41b054c3-04f5-4eed-8aad-91e71f17a07f"
      },
      "source": [
        "### load dataset from tensorflow.keras\n",
        "import tensorflow\n",
        "from tensorflow.keras.datasets import cifar100\n",
        "\n",
        "(X_train, y_train), (X_test, y_test) = cifar100.load_data()"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://www.cs.toronto.edu/~kriz/cifar-100-python.tar.gz\n",
            "169009152/169001437 [==============================] - 2s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AM4pcz_bo5m7"
      },
      "source": [
        "### normalize dataset\n",
        "\n",
        "X_train = X_train / 255.0\n",
        "X_test = X_test / 255.0"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "szi6-IpuzaH1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6eebfc7a-12ed-4eca-93a7-6b8acd104236"
      },
      "source": [
        "### check data shape\n",
        "\n",
        "print(X_train.shape, y_train.shape, X_test.shape, y_test.shape)  \n",
        "\n",
        "### 50000 = train data\n",
        "### 10000 = test data\n",
        "### 60000 = total data\n",
        "### 32 * 32 = colour images' pixel"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(50000, 32, 32, 3) (50000, 1) (10000, 32, 32, 3) (10000, 1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tHbnIjVOo5m8",
        "outputId": "740a4315-16fa-476b-9ab6-5818bd69efe0"
      },
      "source": [
        "### check class\n",
        "import numpy as np\n",
        "\n",
        "np.unique(y_train)\n",
        "\n",
        "### num of class = 100"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16,\n",
              "       17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33,\n",
              "       34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50,\n",
              "       51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67,\n",
              "       68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84,\n",
              "       85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-Pbv55bVo5m8"
      },
      "source": [
        "### load tensorflow library for deep-learning\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Flatten\n",
        "import tensorflow.keras.layers as Layer\n"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U0DUc9qio5m8"
      },
      "source": [
        "### define the model --> Sequential model\n",
        "\n",
        "model = Sequential()   ### load model\n",
        "\n",
        "model.add(Flatten(input_shape = (32, 32, 3)))\n",
        "model.add(Dense(20, input_dim = 100, activation = 'relu'))     ### define nueral net : hidden layer_1\n",
        "model.add(Dense(10, input_dim = 100, activation = 'sigmoid'))  ### define nueral net : hidden layer_2\n",
        "model.add(Dense(8, input_dim = 100, activation = 'relu'))      ### define nueral net : hidden layer_3\n",
        "model.add(Dense(6, input_dim = 100, activation = 'sigmoid'))   ### define nueral net : hidden layer_4\n",
        "model.add(Dense(5, input_dim = 100, activation = 'relu'))      ### define nueral net : hidden layer_5\n",
        "model.add(Dense(4, input_dim = 100, activation = 'relu'))      ### define nueral net : hidden layer_6\n",
        "model.add(Dense(3, input_dim = 100, activation = 'relu'))      ### define nueral net : hidden layer_7\n",
        "model.add(Dense(2, input_dim = 100, activation = 'relu'))      ### define nueral net : hidden layer_8\n",
        "model.add(Dense(2, input_dim = 100, activation = 'sigmoid'))   ### define nueral net : hidden layer_9\n",
        "model.add(Dense(2, input_dim = 100, activation = 'relu'))      ### define nueral net : hidden layer_10\n",
        "model.add(Dense(2, input_dim = 100, activation = 'relu'))      ### define nueral net : hidden layer_11\n",
        "model.add(Dense(2, input_dim = 100, activation = 'relu'))      ### define nueral net : hidden layer_12\n",
        "model.add(Dense(1, activation = 'softmax'))   ### define neral net : output layer "
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y1EcAho2o5m9"
      },
      "source": [
        "### model compiling --> put together all of optimizer, loss, metrics into the model\n",
        "\n",
        "model.compile(optimizer = 'adam', loss = 'categorical_crossentropy', metrics = ['accuracy'])\n"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jDR6a01Lo5m9",
        "outputId": "fa2a5fa2-0723-4928-9af9-635467f20925"
      },
      "source": [
        "### model summary\n",
        "\n",
        "model.summary()"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "flatten (Flatten)            (None, 3072)              0         \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 20)                61460     \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 10)                210       \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 8)                 88        \n",
            "_________________________________________________________________\n",
            "dense_3 (Dense)              (None, 6)                 54        \n",
            "_________________________________________________________________\n",
            "dense_4 (Dense)              (None, 5)                 35        \n",
            "_________________________________________________________________\n",
            "dense_5 (Dense)              (None, 4)                 24        \n",
            "_________________________________________________________________\n",
            "dense_6 (Dense)              (None, 3)                 15        \n",
            "_________________________________________________________________\n",
            "dense_7 (Dense)              (None, 2)                 8         \n",
            "_________________________________________________________________\n",
            "dense_8 (Dense)              (None, 2)                 6         \n",
            "_________________________________________________________________\n",
            "dense_9 (Dense)              (None, 2)                 6         \n",
            "_________________________________________________________________\n",
            "dense_10 (Dense)             (None, 2)                 6         \n",
            "_________________________________________________________________\n",
            "dense_11 (Dense)             (None, 2)                 6         \n",
            "_________________________________________________________________\n",
            "dense_12 (Dense)             (None, 1)                 3         \n",
            "=================================================================\n",
            "Total params: 61,921\n",
            "Trainable params: 61,921\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5hAJbD9jo5m9",
        "outputId": "707fb447-d57d-4342-e45a-f167a99849f3"
      },
      "source": [
        "### fit data on the model\n",
        "\n",
        "results = model.fit(X_train, y_train, epochs = 30, validation_data = (X_test, y_test), batch_size= 128)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/30\n",
            "391/391 [==============================] - 4s 5ms/step - loss: 0.0000e+00 - accuracy: 0.0104 - val_loss: 0.0000e+00 - val_accuracy: 0.0100\n",
            "Epoch 2/30\n",
            "391/391 [==============================] - 2s 4ms/step - loss: 0.0000e+00 - accuracy: 0.0096 - val_loss: 0.0000e+00 - val_accuracy: 0.0100\n",
            "Epoch 3/30\n",
            "391/391 [==============================] - 2s 4ms/step - loss: 0.0000e+00 - accuracy: 0.0099 - val_loss: 0.0000e+00 - val_accuracy: 0.0100\n",
            "Epoch 4/30\n",
            "391/391 [==============================] - 2s 4ms/step - loss: 0.0000e+00 - accuracy: 0.0095 - val_loss: 0.0000e+00 - val_accuracy: 0.0100\n",
            "Epoch 5/30\n",
            "391/391 [==============================] - 2s 4ms/step - loss: 0.0000e+00 - accuracy: 0.0103 - val_loss: 0.0000e+00 - val_accuracy: 0.0100\n",
            "Epoch 6/30\n",
            "391/391 [==============================] - 2s 4ms/step - loss: 0.0000e+00 - accuracy: 0.0098 - val_loss: 0.0000e+00 - val_accuracy: 0.0100\n",
            "Epoch 7/30\n",
            "391/391 [==============================] - 2s 4ms/step - loss: 0.0000e+00 - accuracy: 0.0105 - val_loss: 0.0000e+00 - val_accuracy: 0.0100\n",
            "Epoch 8/30\n",
            "391/391 [==============================] - 2s 4ms/step - loss: 0.0000e+00 - accuracy: 0.0091 - val_loss: 0.0000e+00 - val_accuracy: 0.0100\n",
            "Epoch 9/30\n",
            "391/391 [==============================] - 2s 4ms/step - loss: 0.0000e+00 - accuracy: 0.0095 - val_loss: 0.0000e+00 - val_accuracy: 0.0100\n",
            "Epoch 10/30\n",
            "391/391 [==============================] - 2s 4ms/step - loss: 0.0000e+00 - accuracy: 0.0106 - val_loss: 0.0000e+00 - val_accuracy: 0.0100\n",
            "Epoch 11/30\n",
            "391/391 [==============================] - 2s 4ms/step - loss: 0.0000e+00 - accuracy: 0.0098 - val_loss: 0.0000e+00 - val_accuracy: 0.0100\n",
            "Epoch 12/30\n",
            "391/391 [==============================] - 2s 4ms/step - loss: 0.0000e+00 - accuracy: 0.0103 - val_loss: 0.0000e+00 - val_accuracy: 0.0100\n",
            "Epoch 13/30\n",
            "391/391 [==============================] - 2s 4ms/step - loss: 0.0000e+00 - accuracy: 0.0106 - val_loss: 0.0000e+00 - val_accuracy: 0.0100\n",
            "Epoch 14/30\n",
            "391/391 [==============================] - 2s 4ms/step - loss: 0.0000e+00 - accuracy: 0.0104 - val_loss: 0.0000e+00 - val_accuracy: 0.0100\n",
            "Epoch 15/30\n",
            "391/391 [==============================] - 2s 4ms/step - loss: 0.0000e+00 - accuracy: 0.0106 - val_loss: 0.0000e+00 - val_accuracy: 0.0100\n",
            "Epoch 16/30\n",
            "391/391 [==============================] - 2s 4ms/step - loss: 0.0000e+00 - accuracy: 0.0099 - val_loss: 0.0000e+00 - val_accuracy: 0.0100\n",
            "Epoch 17/30\n",
            "391/391 [==============================] - 2s 4ms/step - loss: 0.0000e+00 - accuracy: 0.0092 - val_loss: 0.0000e+00 - val_accuracy: 0.0100\n",
            "Epoch 18/30\n",
            "391/391 [==============================] - 2s 4ms/step - loss: 0.0000e+00 - accuracy: 0.0099 - val_loss: 0.0000e+00 - val_accuracy: 0.0100\n",
            "Epoch 19/30\n",
            "391/391 [==============================] - 2s 4ms/step - loss: 0.0000e+00 - accuracy: 0.0097 - val_loss: 0.0000e+00 - val_accuracy: 0.0100\n",
            "Epoch 20/30\n",
            "391/391 [==============================] - 2s 4ms/step - loss: 0.0000e+00 - accuracy: 0.0107 - val_loss: 0.0000e+00 - val_accuracy: 0.0100\n",
            "Epoch 21/30\n",
            "391/391 [==============================] - 2s 4ms/step - loss: 0.0000e+00 - accuracy: 0.0096 - val_loss: 0.0000e+00 - val_accuracy: 0.0100\n",
            "Epoch 22/30\n",
            "391/391 [==============================] - 2s 4ms/step - loss: 0.0000e+00 - accuracy: 0.0098 - val_loss: 0.0000e+00 - val_accuracy: 0.0100\n",
            "Epoch 23/30\n",
            "391/391 [==============================] - 2s 4ms/step - loss: 0.0000e+00 - accuracy: 0.0101 - val_loss: 0.0000e+00 - val_accuracy: 0.0100\n",
            "Epoch 24/30\n",
            "391/391 [==============================] - 2s 4ms/step - loss: 0.0000e+00 - accuracy: 0.0098 - val_loss: 0.0000e+00 - val_accuracy: 0.0100\n",
            "Epoch 25/30\n",
            "391/391 [==============================] - 2s 4ms/step - loss: 0.0000e+00 - accuracy: 0.0094 - val_loss: 0.0000e+00 - val_accuracy: 0.0100\n",
            "Epoch 26/30\n",
            "391/391 [==============================] - 2s 4ms/step - loss: 0.0000e+00 - accuracy: 0.0096 - val_loss: 0.0000e+00 - val_accuracy: 0.0100\n",
            "Epoch 27/30\n",
            "391/391 [==============================] - 2s 4ms/step - loss: 0.0000e+00 - accuracy: 0.0101 - val_loss: 0.0000e+00 - val_accuracy: 0.0100\n",
            "Epoch 28/30\n",
            "391/391 [==============================] - 2s 4ms/step - loss: 0.0000e+00 - accuracy: 0.0097 - val_loss: 0.0000e+00 - val_accuracy: 0.0100\n",
            "Epoch 29/30\n",
            "391/391 [==============================] - 2s 4ms/step - loss: 0.0000e+00 - accuracy: 0.0101 - val_loss: 0.0000e+00 - val_accuracy: 0.0100\n",
            "Epoch 30/30\n",
            "391/391 [==============================] - 2s 4ms/step - loss: 0.0000e+00 - accuracy: 0.0100 - val_loss: 0.0000e+00 - val_accuracy: 0.0100\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        },
        "id": "0ae2uOi1o5m-",
        "outputId": "7f95fa92-d424-4ca4-dfbe-386533349c5b"
      },
      "source": [
        "### changes of loss\n",
        "from matplotlib import pyplot as plt\n",
        "\n",
        "plt.plot(results.history['loss'])\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        },
        "id": "M0y-ttYPo5m-",
        "outputId": "27be8e9e-53a1-4c47-dc8c-931a47eeeab3"
      },
      "source": [
        "### changes of accuracy\n",
        "\n",
        "plt.plot(results.history['accuracy'])\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GnDae4UIo5m-"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zv_3xNMjzdLI"
      },
      "source": [
        "## Stretch Goals:\n",
        "\n",
        "- **하이퍼 파라미터 튜닝**을 사용하여 모델의 정확도를 최대한 높여보세요.\n",
        "- **교차 검증(CV) 기법을 사용**하여 모델과 조금 더 일관된 결과를 얻어보세요.\n",
        "- 아직 이론을 배우진 않았지만, **케라스의 CNN 예제**를 **파일만 입력할 수 있다면 사용해볼 수 있는 코드**를 작성해 드렸습니다. **파라미터에 대한 코드를 찾아서 변환**하는 것이기 때문에 CNN이라고 해서 어려울 것은 없습니다. 아직 원리를 모르지만, **두개의 데이터 셋 중 하나를 선택**(cifar100 또는 fashion-MNIST)해서 CNN 모델을 구축하고 파라미터 튜닝을 하면서 결과를 비교해 보십시오. 이론적인 도움이 필요하다고 생각되면, Warmup 영상을 확인하시면 도움을 받을 수 있을 것입니다. \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5RdyQ2P_lFlX"
      },
      "source": [
        "from tensorflow.keras.datasets import fashion_mnist\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout, Flatten\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "\n",
        "batch_size = 128\n",
        "num_classes = 10\n",
        "epochs = 12"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gE04b94vlKvR",
        "outputId": "a8f5515d-f4e2-4378-9c48-bb03c2db72ae"
      },
      "source": [
        "img_rows, img_cols = 28, 28\n",
        "\n",
        "(x_train, y_train), (x_test, y_test) = fashion_mnist.load_data()\n",
        "\n",
        "x_train = x_train.reshape(x_train.shape[0], img_rows, img_cols, 1)\n",
        "x_test = x_test.reshape(x_test.shape[0], img_rows, img_cols, 1)\n",
        "input_shape = (img_rows, img_cols, 1)\n",
        "\n",
        "\n",
        "x_train = x_train.astype('float32')\n",
        "x_test = x_test.astype('float32')\n",
        "x_train /= 255\n",
        "x_test /= 255\n",
        "\n",
        "\n",
        "y_train = to_categorical(y_train, num_classes)\n",
        "y_test = to_categorical(y_test, num_classes)\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Conv2D(32, kernel_size=(3, 3),\n",
        "                 activation='relu',\n",
        "                 input_shape=input_shape))\n",
        "model.add(Conv2D(64, (3, 3), activation='relu'))\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "model.add(Dropout(0.25))\n",
        "model.add(Flatten())\n",
        "model.add(Dense(128, activation='relu'))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(num_classes, activation='softmax'))\n",
        "\n",
        "model.compile(loss='categorical_crossentropy',\n",
        "              optimizer='adam',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "\n",
        "datagen = ImageDataGenerator(\n",
        "        rotation_range=40,\n",
        "        width_shift_range=0.2,\n",
        "        height_shift_range=0.2,\n",
        "        shear_range=0.2,\n",
        "        zoom_range=0.2,\n",
        "        fill_mode='nearest')\n"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-labels-idx1-ubyte.gz\n",
            "32768/29515 [=================================] - 0s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-images-idx3-ubyte.gz\n",
            "26427392/26421880 [==============================] - 0s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-labels-idx1-ubyte.gz\n",
            "8192/5148 [===============================================] - 0s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-images-idx3-ubyte.gz\n",
            "4423680/4422102 [==============================] - 0s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SWm3MdRLlToQ",
        "outputId": "603c3133-072d-4b5a-a07d-e44c8f5b0e4b"
      },
      "source": [
        "model.fit(datagen.flow(x_train, y_train, batch_size=batch_size),\n",
        "                    steps_per_epoch=x_train.shape[0] // batch_size,\n",
        "                    validation_data=(x_test, y_test),\n",
        "                    epochs=epochs, verbose=2)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/12\n",
            "468/468 - 19s - loss: 1.2473 - accuracy: 0.5430 - val_loss: 0.7038 - val_accuracy: 0.7251\n",
            "Epoch 2/12\n",
            "468/468 - 14s - loss: 0.9472 - accuracy: 0.6463 - val_loss: 0.6215 - val_accuracy: 0.7598\n",
            "Epoch 3/12\n",
            "468/468 - 14s - loss: 0.8635 - accuracy: 0.6781 - val_loss: 0.6478 - val_accuracy: 0.7446\n",
            "Epoch 4/12\n",
            "468/468 - 14s - loss: 0.8126 - accuracy: 0.6931 - val_loss: 0.6055 - val_accuracy: 0.7562\n",
            "Epoch 5/12\n",
            "468/468 - 14s - loss: 0.7783 - accuracy: 0.7071 - val_loss: 0.6392 - val_accuracy: 0.7502\n",
            "Epoch 6/12\n",
            "468/468 - 14s - loss: 0.7564 - accuracy: 0.7129 - val_loss: 0.5562 - val_accuracy: 0.7863\n",
            "Epoch 7/12\n",
            "468/468 - 14s - loss: 0.7316 - accuracy: 0.7256 - val_loss: 0.5689 - val_accuracy: 0.7917\n",
            "Epoch 8/12\n",
            "468/468 - 14s - loss: 0.7152 - accuracy: 0.7296 - val_loss: 0.5571 - val_accuracy: 0.7796\n",
            "Epoch 9/12\n",
            "468/468 - 14s - loss: 0.7008 - accuracy: 0.7365 - val_loss: 0.5181 - val_accuracy: 0.8080\n",
            "Epoch 10/12\n",
            "468/468 - 14s - loss: 0.6870 - accuracy: 0.7413 - val_loss: 0.5247 - val_accuracy: 0.8060\n",
            "Epoch 11/12\n",
            "468/468 - 14s - loss: 0.6728 - accuracy: 0.7503 - val_loss: 0.5070 - val_accuracy: 0.8183\n",
            "Epoch 12/12\n",
            "468/468 - 14s - loss: 0.6561 - accuracy: 0.7538 - val_loss: 0.5637 - val_accuracy: 0.7950\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7faf35f093c8>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iJ9g3JTVl9bI",
        "outputId": "0f88eedb-9cd1-49ff-87cb-a89b36f675a2"
      },
      "source": [
        "score = model.evaluate(x_test, y_test, verbose=2)\n",
        "print('Test loss:', score[0])\n",
        "print('Test accuracy:', score[1])\n",
        "\n",
        "\n",
        "model.save('/content/drive/Colab notebooks/section4/sprint2/model.h5')"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "313/313 - 1s - loss: 0.5637 - accuracy: 0.7950\n",
            "Test loss: 0.5637403726577759\n",
            "Test accuracy: 0.7950000166893005\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BmnCyZcvo5nB"
      },
      "source": [
        "# pip install google-colab"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 324
        },
        "id": "prY_fTP6nOjM",
        "outputId": "20b9a541-7cba-4147-f9dc-f620dee68cd4"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "### 이 에러는 어떻게 해결해야 저도 실행할 수 있을까요?"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-18-a145c0899d7d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolab\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdrive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdrive\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforce_remount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36mmount\u001b[0;34m(mountpoint, force_remount, timeout_ms, use_metadata_server)\u001b[0m\n\u001b[1;32m    158\u001b[0m       \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Mountpoint must not be a symlink'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    159\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0m_os\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmountpoint\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0m_os\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmountpoint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 160\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Mountpoint must not already contain files'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    161\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0m_os\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmountpoint\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0m_os\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmountpoint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    162\u001b[0m       \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Mountpoint must either be a directory or not exist'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Mountpoint must not already contain files"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZdvPyfFynWWV",
        "outputId": "af6177fd-63f0-4a81-90a6-e448fd994e0a"
      },
      "source": [
        "# 여러분의 이미지를 입력해야 합니다. \n",
        "# 구글 드라이브의 연동을 활용해보세요.\n",
        "!ls \"drive/my-drive/sc42x/pic1.jpg\""
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "ls: cannot access 'drive/my-drive/sc42x/pic1.jpg': No such file or directory\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p_vuCtvkl558"
      },
      "source": [
        "import cv2 as cv\n",
        "import numpy as np\n",
        "from tensorflow.keras.models import load_model\n",
        "from google.colab.patches import cv2_imshow\n",
        "\n",
        "# 여러분의 이미지의 위치를 입력해야 합니다. \n",
        "img_color = cv.imread(\"drive/my-drive/sc42x/pic1.jpg\", cv.IMREAD_COLOR)\n",
        "img_gray = cv.cvtColor(img_color, cv.COLOR_BGR2GRAY)\n",
        "\n",
        "\n",
        "ret,img_binary = cv.threshold(img_gray, 0, 255, cv.THRESH_BINARY_INV | cv.THRESH_OTSU)\n",
        "\n",
        "kernel = cv.getStructuringElement( cv.MORPH_RECT, ( 5, 5 ) )\n",
        "img_binary = cv.morphologyEx(img_binary, cv. MORPH_CLOSE, kernel)\n",
        "\n",
        "cv2_imshow(img_binary)\n",
        "\n",
        "contours, hierarchy = cv.findContours(img_binary, cv.RETR_EXTERNAL, \n",
        "                        cv.CHAIN_APPROX_SIMPLE)\n",
        "\n",
        "for contour in contours[0:10]:\n",
        "\n",
        "    x, y, w, h = cv.boundingRect(contour)\n",
        "\n",
        "    length = max(w, h) + 60\n",
        "    img_digit = np.zeros((length, length, 1),np.uint8)\n",
        "\n",
        "    new_x,new_y = x-(length - w)//2, y-(length - h)//2\n",
        "\n",
        "\n",
        "    img_digit = img_binary[new_y:new_y+length, new_x:new_x+length]\n",
        "\n",
        "    kernel = np.ones((5, 5), np.uint8)\n",
        "    img_digit = cv.morphologyEx(img_digit, cv.MORPH_DILATE, kernel)\n",
        "\n",
        "    cv2_imshow(img_digit)\n",
        "    # cv.waitKey(0)\n",
        "\n",
        "    model = load_model('model.h5')\n",
        "\n",
        "    img_digit = cv.resize(img_digit, (28, 28), interpolation=cv.INTER_AREA)\n",
        "\n",
        "    img_digit = img_digit / 255.0\n",
        "\n",
        "    img_input = img_digit.reshape(1, 28, 28, 1)\n",
        "    predictions = model.predict(img_input)\n",
        "\n",
        "\n",
        "    number = np.argmax(predictions)\n",
        "    print(number)\n",
        "\n",
        "    cv.rectangle(img_color, (x, y), (x+w, y+h), (255, 255, 0), 2)\n",
        "\n",
        "\n",
        "    location = (x + int(w *0.5), y - 10)\n",
        "    font = cv.FONT_HERSHEY_COMPLEX  \n",
        "    fontScale = 1.2\n",
        "    cv.putText(img_color, str(number), location, font, fontScale, (0,255,0), 2)\n",
        "    \n",
        "\n",
        "    cv2_imshow(img_digit)\n",
        "    # cv.waitKey(0)\n",
        "    \n",
        "\n",
        "cv2_imshow(img_color)\n",
        "cv.waitKey(0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mZXJqlbSpKyf"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}